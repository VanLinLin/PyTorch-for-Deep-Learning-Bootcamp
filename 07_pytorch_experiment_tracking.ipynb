{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. PyTorch Experiment Tracking\n",
    "\n",
    "Machine learning is very experimental.\n",
    "\n",
    "In order to figure out which experiment are worth purshing, that's where **experiment tracking** comes in, it helps you to figure out what doesn't work so you can figure out what **does** work.\n",
    "\n",
    "In this notebook, we're going to see an example of programmatically tracking experiments.\n",
    "\n",
    "Resources:\n",
    "* [Book version of notebook](https://www.learnpytorch.io/07_pytorch_experiment_tracking/)\n",
    "* [Ask a question](https://github.com/mrdbourke/pytorch-deep-learning/discussions)\n",
    "* [Extra-curriculum](https://madewithml.com/courses/mlops/experiment-tracking/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.13.0+cu117', '0.14.0+cu117')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int = 42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data\n",
    "\n",
    "Want to get pizza, steak, sushi images.\n",
    "\n",
    "So we can run experiments building FoodVision Mini and see which model performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Example source: https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n",
    "def download_data(source: str,\n",
    "                  destination: str,\n",
    "                  remove_source: bool = True) -> Path:\n",
    "    \"\"\"Download a zipped data set from source and unzip to destinaiton.\"\"\"\n",
    "    # Setup path to data folder\n",
    "    data_path = Path('data/')\n",
    "    image_path = data_path / destination\n",
    "\n",
    "    # If image folder doesn't exist, creat it\n",
    "    if image_path.is_dir():\n",
    "        print(f'[INFO] {image_path} already exist, skipping download')\n",
    "    else:\n",
    "        print(f'[INFO] {image_path} does not exist, creating')\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Download the target data\n",
    "        target_file = Path(source).name\n",
    "        with open(data_path / target_file, 'wb') as f:\n",
    "            request = requests.get(source)\n",
    "            print(f'[INFO] Downloading {target_file} from {source}')\n",
    "            f.write(request.content)\n",
    "\n",
    "        # Unzip target file\n",
    "        with zipfile.ZipFile(data_path / target_file, 'r') as f:\n",
    "            print(f'[INFO] Unzipping {target_file} data')\n",
    "            f.extractall(image_path)\n",
    "\n",
    "        # Remove zip file\n",
    "        if remove_source:\n",
    "            os.remove(data_path / target_file)\n",
    "\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data\\pizza_steak_sushi already exist, skipping download\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = download_data(source='https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip',\n",
    "                           destination='pizza_steak_sushi')\n",
    "\n",
    "image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create DataSets and DataLoaders\n",
    "\n",
    "### 2.1 Create DataLoaders with manual transforms\n",
    "\n",
    "The goal with transforms is to ensure your custom data is transformed in a reproducible way as well as a way that will suit pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup a directories\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually created transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x17f3b902860>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x17f3b903400>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup ImageNet normalization levels\n",
    "# See here: https://pytorch.org/vision/stable/models.html\n",
    "from going_modular.data_setup import create_dataloaders\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[.485, .456, .406],\n",
    "                                 std=[.229, .224, .225])\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "print(f'Manually created transforms: {manual_transforms}')\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=manual_transforms, batch_size=32)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create DataLoaders using automatically created transforms\n",
    "\n",
    "The same principle applies for automatic transforms: we want our custom data in the same format as a pretrained model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created transforms: ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x17f3b902620>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x17f3b902b60>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup directories\n",
    "import torchvision\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "\n",
    "# Setup pretrained weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # \"DEFAULT\" = best available\n",
    "\n",
    "# Get transforms from weights (these are the transforms used to train a particular or obtain a particular set of weights)\n",
    "automatic_transforms = weights.transforms()\n",
    "print(f'Automatically created transforms: {automatic_transforms}')\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=automatic_transforms, batch_size=32)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting a pretrained model, freeze the base layers and change the classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained weights for efficientNet_B0\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "# Setup the model with the pretrained weights and send it to the target device\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free all base layers by setting their required_grad attribute to False\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chage the classifier hear\n",
    "from torch import nn\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names))\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]         [32, 3]                   --                        Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]         [32, 1280, 7, 7]          --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]         [32, 32, 112, 112]        --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]         [32, 32, 112, 112]        (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]        [32, 32, 112, 112]        (64)                      False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]        [32, 16, 112, 112]        --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]        [32, 16, 112, 112]        (1,448)                   False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]        [32, 24, 56, 56]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]        [32, 24, 56, 56]          (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]          [32, 24, 56, 56]          (10,710)                  False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]          [32, 40, 28, 28]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]          [32, 40, 28, 28]          (15,350)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]          [32, 40, 28, 28]          (31,290)                  False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]          [32, 80, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]          [32, 80, 14, 14]          (37,130)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]          [32, 112, 14, 14]         --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]          [32, 112, 14, 14]         (126,004)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]         [32, 192, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]         [32, 192, 7, 7]           (262,492)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]           [32, 320, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]           [32, 320, 7, 7]           (717,232)                 False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]           [32, 1280, 7, 7]          --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]           [32, 1280, 7, 7]          (409,600)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]          [32, 1280, 7, 7]          (2,560)                   False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]          [32, 1280, 7, 7]          --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]          [32, 1280, 1, 1]          --                        --\n",
       "├─Sequential (classifier)                                    [32, 1280]                [32, 3]                   --                        True\n",
       "│    └─Dropout (0)                                           [32, 1280]                [32, 1280]                --                        --\n",
       "│    └─Linear (1)                                            [32, 1280]                [32, 3]                   3,843                     True\n",
       "================================================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 12.31\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        verbose=0,\n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a single model and track results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To track experiments, we're going to use [TensorBoard](https://www.tensorflow.org/tensorboard?hl=zh-tw)\n",
    "\n",
    "And to interact with TensorBoard, we can use PyTorch's [SummaryWriter](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "  * Also see [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x17f43573970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup a SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'train_step' from partially initialized module 'going_modular.engine' (most likely due to a circular import) (d:\\Side_project\\Udemy\\PyTorch_for_Deep_Learning_in_2023\\going_modular\\engine.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, List, Tuple\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoing_modular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m train_step, test_step\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m      7\u001b[0m           train_dataloader: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader,\n\u001b[0;32m      8\u001b[0m           test_dataloader: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m           epochs: \u001b[39mint\u001b[39m,\n\u001b[0;32m     12\u001b[0m           device: torch\u001b[39m.\u001b[39mdevice) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, List[\u001b[39mfloat\u001b[39m]]:\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\"\"Trains and tests a PyTorch model.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m    Passes a target PyTorch models through train_step() and test_step()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m                    test_acc: [0.3400, 0.2973]} \u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Side_project\\Udemy\\PyTorch_for_Deep_Learning_in_2023\\going_modular\\engine.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, List, Tuple\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoing_modular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m train_step, test_step\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m      8\u001b[0m           train_dataloader: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader,\n\u001b[0;32m      9\u001b[0m           test_dataloader: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m           epochs: \u001b[39mint\u001b[39m,\n\u001b[0;32m     13\u001b[0m           device: torch\u001b[39m.\u001b[39mdevice) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, List[\u001b[39mfloat\u001b[39m]]:\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\"\"Trains and tests a PyTorch model.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[39m    Passes a target PyTorch models through train_step() and test_step()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m                    test_acc: [0.3400, 0.2973]} \u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'train_step' from partially initialized module 'going_modular.engine' (most likely due to a circular import) (d:\\Side_project\\Udemy\\PyTorch_for_Deep_Learning_in_2023\\going_modular\\engine.py)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from going_modular.engine import train_step, test_step\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                    train_acc: [...],\n",
    "                    test_loss: [...],\n",
    "                    test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "                   {train_loss: [2.0616, 1.0537],\n",
    "                    train_acc: [0.3945, 0.3945],\n",
    "                    test_loss: [1.2641, 1.5706],\n",
    "                    test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "               }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        ### New: Experiment tracking ###\n",
    "        writer.add_scalars(main_tag='Loss',\n",
    "                           tag_scalar_dict={'train_loss': train_loss,\n",
    "                                            'test_loss': test_loss},\n",
    "                           global_step=epoch)\n",
    "\n",
    "        writer.add_scalars(main_tag='Accuracy',\n",
    "                           tag_scalar_dict={'train_acc': train_acc,\n",
    "                                            'test_acc': test_acc},\n",
    "                           global_step=epoch)\n",
    "\n",
    "        writer.add_graph(model=model,\n",
    "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "\n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    ### End new ###\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd59ff662a2c4d42875da4806949274b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0744 | train_acc: 0.4219 | test_loss: 0.8657 | test_acc: 0.7737\n",
      "Epoch: 2 | train_loss: 0.8918 | train_acc: 0.6641 | test_loss: 0.7681 | test_acc: 0.7945\n",
      "Epoch: 3 | train_loss: 0.7432 | train_acc: 0.7500 | test_loss: 0.6500 | test_acc: 0.8655\n",
      "Epoch: 4 | train_loss: 0.6621 | train_acc: 0.8867 | test_loss: 0.6412 | test_acc: 0.8456\n",
      "Epoch: 5 | train_loss: 0.6875 | train_acc: 0.7617 | test_loss: 0.6665 | test_acc: 0.8144\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "# Note: not using engine.train(), since we updated the train() function above\n",
    "set_seeds()\n",
    "\n",
    "results = train(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=5,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View our model's results with TensorBoard\n",
    "\n",
    "There are a few ways to view TensorBoard results, see them [here](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#5-view-our-models-results-in-tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 25292), started 0:00:27 ago. (Use '!kill 25292' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4873dbb1d6c40d77\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4873dbb1d6c40d77\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's view our experiments from within the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a function to prepare a `SummaryWriter()` instance\n",
    "\n",
    "By default our `SummaryWriter()` saves to `log_dir`.\n",
    "\n",
    "How about if we wanted to save different experiments to different folders?\n",
    "\n",
    "In essence, one experiment = one folder\n",
    "\n",
    "For example, we'd like to track:\n",
    "* Experiment date/timestep\n",
    "* Experiment name\n",
    "* Model name\n",
    "* Extra - is there anything else that should be tracked?\n",
    "\n",
    "Let's create a function to create a `SummaryWriter()` instance to take all of these things into account.\n",
    "\n",
    "So ideally we end up tracking experiments to a directory:\n",
    "\n",
    "`run/YYYY-MM-DD/experiment_name/model_name/extra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str = None):\n",
    "    \"\"\"Creat a torch.utils.tensorboard.writer.SummaryWriter() instance tracking to a specific directory.\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date in reverse order\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join('runs', timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join('runs', timestamp, experiment_name, model_name)\n",
    "\n",
    "    print(f'[INFO] Created SummaryWriter saving to {log_dir}')\n",
    "\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter saving to runs\\2023-07-17\\data_10_percent\\effnetb0\\5_epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x1c85fc410c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_writer = create_writer(experiment_name='data_10_percent',\n",
    "                               model_name='effnetb0',\n",
    "                               extra='5_epochs')\n",
    "\n",
    "example_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Update the `train()` function to include a `writer` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from going_modular.engine import train_step, test_step\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                    train_acc: [...],\n",
    "                    test_loss: [...],\n",
    "                    test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "                   {train_loss: [2.0616, 1.0537],\n",
    "                    train_acc: [0.3945, 0.3945],\n",
    "                    test_loss: [1.2641, 1.5706],\n",
    "                    test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "               }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        ### New: Experiment tracking ###\n",
    "        if writer:\n",
    "            writer.add_scalars(main_tag='Loss',\n",
    "                            tag_scalar_dict={'train_loss': train_loss,\n",
    "                                                'test_loss': test_loss},\n",
    "                            global_step=epoch)\n",
    "\n",
    "            writer.add_scalars(main_tag='Accuracy',\n",
    "                            tag_scalar_dict={'train_acc': train_acc,\n",
    "                                                'test_acc': test_acc},\n",
    "                            global_step=epoch)\n",
    "\n",
    "            writer.add_graph(model=model,\n",
    "                            input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "            ### End new ###\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setting up a series of modelling experiments\n",
    "\n",
    "* Setup 2x modelling experiments with effnetb0, pizza, steak, sushi data and train one model for 5 epochs and another model for 10 epochs\n",
    "\n",
    "### 7.1 What kind of experiments should you run?\n",
    "\n",
    "The number of machine learning experiments you can run, is like the number of different models you can build... almost limitless.\n",
    "\n",
    "However, you can't test everything...\n",
    "\n",
    "So what should you test?\n",
    "* Change the number of epochs\n",
    "* Change the number of hidden layer/units\n",
    "* Change the amount of data (right now we're using 10% of the Food101 dataset for pizza, steak, sushi)\n",
    "* Change the learning rate\n",
    "* Try different kinds of data augmentation\n",
    "* Choose a different model architectures\n",
    "\n",
    "This is why transfer learning is so powerful, because, it's a working model that you can apply to your own problem.\n",
    "\n",
    "### 7.2 What experiments are we going to run?\n",
    "\n",
    "We're going to turn three dials:\n",
    "1. Model size - Effnetb0 vs EffnetB2 (in terms of number of parameters)\n",
    "2. Dataset size - 10% of pizza, steak, sushi images vs 20% (generally more data = better results)\n",
    "3. Training time - 5 epochs vs 10 epochs (generally longer training time = better results, up to a point)\n",
    "\n",
    "To begin, we're still keeping things relatively small so that our experiments run quickly.\n",
    "\n",
    "**Our goal:** a model that is well performing but still small enough to run on a mobile device or web browser, so FoodVision Mini can come to life.\n",
    "\n",
    "If you had infinite compute + time, you should basically always choose the biggest model and biggest data you can.\n",
    "\n",
    "\n",
    "### 7.3 Download different dataset\n",
    "\n",
    "We want two datasets:\n",
    "1. [Pizza, steak, sushi 10%](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip)\n",
    "2. [Pizza, steak, sushi 20%](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data\\pizza_steak_sushi already exist, skipping download\n",
      "[INFO] data\\pizza_steak_sushi_20_percent already exist, skipping download\n"
     ]
    }
   ],
   "source": [
    "# Download 10 percent and 20 percent datasets\n",
    "data_10_percent_path = download_data(source='https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip', destination='pizza_steak_sushi')\n",
    "\n",
    "data_20_percent_path = download_data(source='https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip', destination='pizza_steak_sushi_20_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
